### Design Spotify Top K Songs

Background: Typically there are four types of windows in stream processing systems:
tumbling window, sliding window, session window, and global window

Global window: window that covers the entire period of data. In context: global window means top k songs of all time 
Tumbling window: fixed size window that does not slide: In context: tumbling window means top k songs of a predefined time interval 
Sliding window: dynamic window that slides over time. 
 -- Example if we set up a sliding window of 10 minutes wiht a slide interval of 1 minute,
 all events in the last 10 minutes will be in the same window, but every minute, the window will
 slide forward by 1 minute. 
 -- Fixed sliding window: the window size and slide interval are predefined, events are bucketed along a fixed time axis 
 -- Arbitary sliding window: the slide interval is not fixed. The window sizde is predefined. Events are bucketed along a dynamic time axis
 meaning the windows open and close based on when events arrive


How would top K be implemented in production:
- Before designing the system, can we just use Flink to implement the top K aggregator in a sliding window?

Stream Processor Implementation:
In production we would use a stream processor like apache flink, spark streaming, kafka streams, google cloud dataflow, aws kinesis, azure stream analytics, or whatever favorite stream process to implement the top K 
aggregator. It's a very popular technology and there are many providers out there

The typical data flow in a stream processor is to read datat from a stream and apply transformation and aggregations and write the results to a stream.

We would write MapReduce style code to 
- Apply transitions
- Group or partition the data based on keys
- Use windowing logic   

Redis Sorted Set Implementation:

If the question for the top K songs in Global window, we have an even simpler implementation using Redis sorted set


High Level Design: 
Global window Top K, Low QPS 
Heap Algo for Top K with hashmap:

Check if the song is in a hashmap:
If it is not then we check if the view count is smaller then the top of the heap which has the smallest view count in the minheap
otherwise, check if if the videoId exists in the haspmap:
    if it does then update the count in the hash map 
    Re-heapify by performing a bubble down operation, where the element swaps with its children until the heap property is restored.
If the video is not in the heap
    Replace the head of the heap with the new videeo 
    Perform a sift-down operation to maintain the heap property 

But this design has issues:
scalability: the service wont operate as a single instance and keep the min heap in memory. 
It wont be able to handle 120k views/second, so we need a way to scale the service

Fault Tolerance: if the service crashes or is redeployed, the in memeory min heap will be lost. We need a mechanism to 
persist the heap for recovery avoiding the need to replay all events which would be too slow 

Query Flexibility: the service only supports queries for all time top K videos.
It needs to be extended to support querying top k for specific time windows 



Global window Top K, High QPS: need to scale for more users 

How do we partition service into multiple instances to increase the throughput?
Round Robin: 
partition resources throughout equally 
This can lead to inconsistencies as events in same list may be processed in different instances. We need to have more than top K in each partiition as 
the song may not be top K in one partition but top K in global sense it is. 

Fixed Hash: Partition the stream with a song id hash 
Partition  = Hash(song_id) % number of partitions 
Ensures consistency: we do not need to record more than top K songs in each partion: solves the issue with a round robin use 
Easy to implement and scales well 
Good load distribution if song requests are uniformly spread 
Cons: 
Hotspots can occur if a song become extremely popular 
Rebalancing is complex. If number of songs changes then redistributed across the partitions can change 

Dynamic Hash: consistent hashing used to dynamically adjust the number of partiitons 
Technicals
1. Hash the song id,
2. Hash the backend instance, each backend instance is assigned a position on the same hash ring 
3. Event to instance hashing: events are routed to nearest instance on the clockwise ring. This means each 
instance is repsonsible for the range of events between itself and the previous instance on the ring 
4. Handling instance changes: when a backend instance is added or removed 
only the range of events between the removed instance and its neighbors are effected 

